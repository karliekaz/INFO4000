{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.io import read_image\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know how to revive my dead kernel. Since all my cell outputs are still shown below I'm jsut going to leave it as is. I hope this is okay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the ResNet18 model because some surface-level research showed that since it is simple CNN with fewer parameters than other models, it faster to train & less prone to overfitting. It also apparently preforms well with the MNIST dataset, training to have high accuracy in only a few epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing model with the best available weights\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 'requires_grad' to false for all parameters in order to freeze layers \n",
    "# (modified from example)\n",
    "\n",
    "for child in model.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I found it necessary to freeze ALL the layers because the convolutional layers of the pre-trained ResNet18 model have already learned useful features that can be used for the MNIST dataset. \n",
    "By freezing them, I can prevent overfitting & also speed up training.\n",
    "\n",
    "Modifying the fully connected layer is necessary, though, so that the model accounts for the 10 classes of the dataset. I also have to change the \"Linear\" layer to be \"Conv.2d\", because \"Linear\" is designed to deal with 1D data, and the MNIST dataset is 2D grayscale image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file names for the training/testing data are in csv files, while the actual images are in the \"Images\" folder.\n",
    "I'm using this modified class to consolidate those & pre-process the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.dnn' has no attribute 'DictValue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m \u001b[39m# Using the computer vision library becuase it makes using images easy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/cv2/__init__.py:181\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[39mif\u001b[39;00m DEBUG: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExtra Python code for\u001b[39m\u001b[39m\"\u001b[39m, submodule, \u001b[39m\"\u001b[39m\u001b[39mis loaded\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m DEBUG: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOpenCV loader: DONE\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m bootstrap()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/cv2/__init__.py:175\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m DEBUG: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOpenCV loader: binary extension... OK\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[39mfor\u001b[39;00m submodule \u001b[39min\u001b[39;00m __collect_extra_submodules(DEBUG):\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mif\u001b[39;00m __load_extra_py_code_for_module(\u001b[39m\"\u001b[39;49m\u001b[39mcv2\u001b[39;49m\u001b[39m\"\u001b[39;49m, submodule, DEBUG):\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m DEBUG: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExtra Python code for\u001b[39m\u001b[39m\"\u001b[39m, submodule, \u001b[39m\"\u001b[39m\u001b[39mis loaded\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m DEBUG: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOpenCV loader: DONE\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/cv2/__init__.py:28\u001b[0m, in \u001b[0;36m__load_extra_py_code_for_module\u001b[0;34m(base, name, enable_debug_print)\u001b[0m\n\u001b[1;32m     26\u001b[0m native_module \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mpop(module_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     py_module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(module_name)\n\u001b[1;32m     29\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m enable_debug_print:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/cv2/typing/__init__.py:169\u001b[0m\n\u001b[1;32m    167\u001b[0m ExtractArgsCallback \u001b[39m=\u001b[39m typing\u001b[39m.\u001b[39mCallable[[typing\u001b[39m.\u001b[39mSequence[GTypeInfo]], typing\u001b[39m.\u001b[39mSequence[GRunArg]]\n\u001b[1;32m    168\u001b[0m ExtractMetaCallback \u001b[39m=\u001b[39m typing\u001b[39m.\u001b[39mCallable[[typing\u001b[39m.\u001b[39mSequence[GTypeInfo]], typing\u001b[39m.\u001b[39mSequence[GMetaArg]]\n\u001b[0;32m--> 169\u001b[0m LayerId \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mdnn\u001b[39m.\u001b[39;49mDictValue\n\u001b[1;32m    170\u001b[0m IndexParams \u001b[39m=\u001b[39m typing\u001b[39m.\u001b[39mDict[\u001b[39mstr\u001b[39m, typing\u001b[39m.\u001b[39mUnion[\u001b[39mbool\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]\n\u001b[1;32m    171\u001b[0m SearchParams \u001b[39m=\u001b[39m typing\u001b[39m.\u001b[39mDict[\u001b[39mstr\u001b[39m, typing\u001b[39m.\u001b[39mUnion[\u001b[39mbool\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2.dnn' has no attribute 'DictValue'"
     ]
    }
   ],
   "source": [
    "import cv2 # Using the computer vision library becuase it makes using images easy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# (modified from INFO 3000 MP2)\n",
    "class MNIST():\n",
    "\n",
    "    def __init__(self, data, dir):\n",
    "        self.data = data\n",
    "        self.dir = dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Since the filenames & images are in different places, we have to consolidate them\n",
    "        img_name = os.path.join(self.dir, self.data.iloc[idx, 0]) # Grabbing the image\n",
    "        img = cv2.imread(img_name, cv2.IMREAD_GRAYSCALE) # Using cv2 to read the image\n",
    "        label = self.data.iloc[idx, 1]\n",
    "\n",
    "        # Resizing\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "\n",
    "        # Normalizing and converting to a tensor\n",
    "        img = img / 255.0\n",
    "        img = torch.unsqueeze(torch.FloatTensor(img), 0)  # Add a single channel dimension\n",
    "\n",
    "        label = torch.tensor(int(label))  # Convert label to a tensor\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49000 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing dataset\n",
    "train_csv = pd.read_csv('Mnist_Image_Data/train.csv')\n",
    "test_csv = pd.read_csv('Mnist_Image_Data/test.csv')\n",
    "\n",
    "train_data = MNIST(data=train_csv, dir=\"Mnist_Image_Data/Images/train\")\n",
    "test_data = MNIST(data=test_csv, dir=\"Mnist_Image_Data/Images/test\")\n",
    "\n",
    "# Amount of data - train and test while testing the __len__ method\n",
    "print(train_data.__len__(),test_data.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io,transform\n",
    "from skimage.color import rgb2gray\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Shape of Imagetorch.Size([1, 32, 32]) datatype iamge: <class 'torch.Tensor'> datalabel: <class 'torch.Tensor'>\n",
      "(32, 32)\n",
      "torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (modified from INFO 3000 MP2)\n",
    "# Display an image while testing the __getitem__ method\n",
    "\n",
    "# Get image from dataset\n",
    "image1,label1 = train_data.__getitem__(20779)\n",
    "print(type(label1))\n",
    "print(f\"Shape of Image{image1.shape} datatype iamge: {type(image1)} datalabel: {type(label1)}\")\n",
    "\n",
    "# Reshape image to make it plottable after converting to numpy array\n",
    "image1 = (image1.numpy()).reshape(32,32)\n",
    "\n",
    "# Check the shape of the image\n",
    "print(image1.shape)\n",
    "print(label1.shape)\n",
    "\n",
    "# Display image\n",
    "io.imshow(image1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like __getitem__ works! Now I have to train the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Data Loaders from the tested datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've tried 3, 5, 8, 10, and 20 epochs and have found that 8 has the best accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Loss: 1.2247027193752966\n",
      "Epoch 2/8, Loss: 1.0340786939657076\n",
      "Epoch 3/8, Loss: 0.9962596806160152\n",
      "Epoch 4/8, Loss: 0.9850989089933451\n",
      "Epoch 5/8, Loss: 0.9811189005020082\n",
      "Epoch 6/8, Loss: 0.9803316162865093\n",
      "Epoch 7/8, Loss: 0.9658899850384685\n",
      "Epoch 8/8, Loss: 0.9719990796893446\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 70.0%\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on test set: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% is still not the *best* accuracy, but I am satisfied with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info4000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
